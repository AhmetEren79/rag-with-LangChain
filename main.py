import os
os.environ["PYTHONUTF8"] = "1"
import httpx
def _patched_normalize_header_value(value, encoding: str | None = None) -> bytes:
    if isinstance(value, bytes):
        return value  # EÄŸer zaten bytes ise, dokunma
    return value.encode(encoding or "utf-8") # EÄŸer string ise, utf-8 ile encode et

httpx._models._normalize_header_value = _patched_normalize_header_value

import textwrap
import streamlit as st
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate

load_dotenv()

prompt_template = """Verilen baÄŸlam parÃ§alarÄ±nÄ± kullanarak sondaki soruyu yanÄ±tlayÄ±n. 
CevabÄ± bilmiyorsanÄ±z, bilmediÄŸinizi sÃ¶yleyin, cevap uydurmaya Ã§alÄ±ÅŸmayÄ±n.
CevabÄ±nÄ±zÄ± mutlaka kapsamlÄ± ve aÃ§Ä±klayÄ±cÄ± bir ÅŸekilde TÃ¼rkÃ§e olarak verin.

BaÄŸlam:
{context}

Soru: {question}

YardÄ±mcÄ± Cevap (TÃ¼rkÃ§e):"""

QA_PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

@st.cache_resource
def create_qa_chain(_uploaded_file):

    if _uploaded_file is not None:
        temp_file_path = os.path.join("./", _uploaded_file.name)
        with open(temp_file_path, "wb") as f:
            f.write(_uploaded_file.getbuffer())

        loader = PyPDFLoader(temp_file_path)
        docs = loader.load_and_split()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = text_splitter.split_documents(docs)
        embeddings = OpenAIEmbeddings()
        vector_store = FAISS.from_documents(chunks, embeddings)

        llm = ChatOpenAI(temperature=0, model_name='gpt-4o')

        qa_chain = RetrievalQA.from_chain_type(
            llm,
            retriever=vector_store.as_retriever(),
            chain_type_kwargs={"prompt": QA_PROMPT}
        )
        return qa_chain
    return None

st.set_page_config(page_title="PDF ile Sohbet", page_icon="ğŸ“„")

st.title("ğŸ“„ PDF DosyanÄ±zla Sohbet Edin")
st.write("Bir PDF dosyasÄ± yÃ¼kleyin ve iÃ§eriÄŸi hakkÄ±nda sorular sorun.")

uploaded_file = st.file_uploader("LÃ¼tfen PDF dosyanÄ±zÄ± buraya yÃ¼kleyin", type="pdf")


if uploaded_file is not None:
    with st.spinner('PDF iÅŸleniyor, vektÃ¶rler oluÅŸturuluyor... LÃ¼tfen bekleyin.'):
        qa_chain = create_qa_chain(uploaded_file)
        st.success(f"**{uploaded_file.name}** baÅŸarÄ±yla iÅŸlendi! ArtÄ±k soru sorabilirsiniz.")


    question = st.text_input("PDF iÃ§eriÄŸi hakkÄ±nda bir soru sorun:", placeholder="Ã–rn: Yapay zekanÄ±n sÄ±kÄ±ntÄ±larÄ± nelerdir?")

    if question:
        with st.spinner('Cevap aranÄ±yor...'):
            try:
                answer = qa_chain.invoke(question)
                result_text = answer.get('result', 'Cevap alÄ±namadÄ±.')

                st.markdown("### Cevap:")
                st.info(result_text)

            except Exception as e:
                st.error(f"Bir hata oluÅŸtu: {e}")

st.sidebar.header("NasÄ±l Ã‡alÄ±ÅŸÄ±r?")
st.sidebar.markdown("""
1.  **PDF YÃ¼kle:** `GÃ¶zat` butonu ile bilgisayarÄ±nÄ±zdan bir PDF dosyasÄ± seÃ§in.
2.  **Ä°ÅŸleme:** Uygulama, PDF'in metnini Ã§Ä±karÄ±r, anlamsal parÃ§alara ayÄ±rÄ±r ve LangChain ile bir vektÃ¶r veritabanÄ± oluÅŸturur.
3.  **Soru Sor:** Metin kutusuna sorunuzu yazÄ±p `Enter`'a basÄ±n.
4.  **Cevap Al:** Yapay zeka, sorunuza en uygun cevabÄ± PDF iÃ§eriÄŸinden bularak size sunar.
""")
st.sidebar.markdown("---")
st.sidebar.write("Ahmet Eren tarafÄ±ndan geliÅŸtirildi.")